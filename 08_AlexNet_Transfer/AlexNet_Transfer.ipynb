{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e990e6801202c40d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/bmalcover/aa_2425/blob/main/08_AlexNet_Transfer/AlexNet_Transfer.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "272d21b93244ea6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T16:24:07.261921Z",
     "start_time": "2024-10-30T16:24:01.415725Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\miniconda3\\envs\\ia_2024\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457b4be692d2233a",
   "metadata": {},
   "source": [
    "# Models ja existents i *transfer learning*\n",
    "\n",
    "En aquesta pràctica, aprofundirem en la classificació d’imatges amb xarxes neuronals convolucionals (CNNs), però amb un enfocament diferent al de la sessió anterior. Mentre que anteriorment vàrem construir CNNs des de zero per comprendre la seva estructura bàsica, aquest cop treballarem amb un model de CNN preentrenat: **AlexNet**. Els objectius són:\n",
    "\n",
    "- **Comprendre i utilitzar un model existent**: en aquest cas, AlexNet, un model ja entrenat sobre un gran conjunt de dades.\n",
    "- **Transfer Learning**: aprendre com aprofitar els coneixements d’una xarxa preentrenada i adaptar-la per resoldre una nova tasca.\n",
    "- **Càrrega de conjunts de dades d’imatges locals**: aplicar el processament d’imatges i la càrrega de dades des de l’ordinador.\n",
    "\n",
    "Aquest enfocament ajuda a optimitzar l’entrenament i és especialment útil quan es disposa de pocs recursos computacionals o un conjunt de dades més reduït.\n",
    "\n",
    "### AlexNet\n",
    "\n",
    "AlexNet és una xarxa que va establir un nou estàndard en visió per computador gràcies a la seva capacitat de reconeixement en múltiples categories. La seva estructura, composta de capes convolucionals i de max-pooling, amb capes totalment connectades al final, és una referència en el camp de les CNNs.\n",
    "\n",
    "Aquest cop, usarem AlexNet com a punt de partida, sense construir el model des de zero, per veure com es poden reutilitzar i adaptar les seves característiques apreses per a nous conjunts de dades.\n",
    "\n",
    "### Què és el Transfer Learning?\n",
    "\n",
    "El Transfer Learning és una tècnica que permet aprofitar les xarxes preentrenades (com AlexNet) per a una nova tasca. La xarxa es modifica per adaptar-la a les noves classes del conjunt de dades que volem classificar, fent ús de les característiques generals ja apreses en l’entrenament inicial (vores, textures, etc.).\n",
    "\n",
    "Aquest process es pot fer de dues maneres. La primera és el que també rep el nom de **fine-tunning**:\n",
    "- Congelarem les primeres capes del model per conservar les característiques generals apreses.\n",
    "- Modificarem i entrenarem només les capes finals per adaptar-les a les noves classes, fent que el model s’ajusti de forma ràpida i amb menys dades.\n",
    "\n",
    "La segona, que anomenam també com la categoria general **transfer learning**:\n",
    "\n",
    "- Congelarem les capes de l'extractor de característiques del model per conservar les característiques generals apreses.\n",
    "- Afegir un nou classificador ``MLP`` i entrenar-ho de 0.\n",
    "\n",
    "\n",
    "Aquest procediment permetrà entendre com es pot utilitzar una xarxa ja existent per resoldre tasques específiques sense haver de construir ni entrenar un model completament des de zero.\n",
    "\n",
    "### Guia de la Pràctica\n",
    "\n",
    "En aquest notebook treballarem per:\n",
    "\n",
    " 1. Carregar i preparar un conjunt d’imatges des de l’ordinador.\n",
    " 2. Utilitzar el model AlexNet preentrenat i aplicar transfer learning per ajustar-lo a noves categories.\n",
    " 3. Analitzar el rendiment del model i visualitzar els resultats.\n",
    "\n",
    "## Començam\n",
    "\n",
    "Primer de tot, com sempre, hem d'obtenir les dades. Aquesta sessió la farem amb el conjunt de dades [Tiny ImageNet](https://www.kaggle.com/c/tiny-imagenet/data?select=train.images.zip). \n",
    "\n",
    "Aquest conjunt de dades es defineix en la seva plana de la forma següent:\n",
    "\n",
    "> MicroImageNet classification challenge is similar to the classification challenge in the full ImageNet ILSVRC. MicroImageNet contains 200 classes for training. Each class has 500 images. The test set contains 10,000 images. All images are 64x64 colored ones.\n",
    "\n",
    "Aquesta vegada no farem feina amb un conjunt de dades ja existents a ``torchvision`` sinó que nosaltres farem la gestió des de 0. Per tant i primer de tot descarregarem les dades. Per fer-ho podem trobar el conjunt de dades a la següent plana (http://cs231n.stanford.edu/tiny-imagenet-200.zip).\n",
    "\n",
    "Alternativament també podem emprar l'eina ``wget`` així:\n",
    "\n",
    "```\n",
    "wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "```\n",
    "\n",
    "Una vegada que hem descarregat les dades les podem descomprimir i finalment comença a fer-hi feina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f040a135483ff08e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T16:24:07.899304Z",
     "start_time": "2024-10-30T16:24:07.279931Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 5\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train = datasets.ImageFolder('../data/tiny-imagenet/train', transform=transform)\n",
    "test = datasets.ImageFolder('../data/tiny-imagenet/test', transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fa3a6b8385d7",
   "metadata": {},
   "source": [
    "L'estructura ``ImageFolder`` de PyTorch és una classe de ``torchvision.datasets`` que permet carregar un conjunt de dades d'imatges estructurat en carpetes, on cada subcarpeta representa una classe. És útil per a projectes de classificació d'imatges, ja que facilita la lectura automàtica d'imatges i etiquetes a partir de la seva organització en el sistema de fitxers.\n",
    "\n",
    "### Estructura de carpetes\n",
    "L'estructura que requereix ``ImageFolder`` per funcionar és la següent:\n",
    "\n",
    "```\n",
    "dataset_root/\n",
    "├── class1/\n",
    "│   ├── image1.jpg\n",
    "│   ├── image2.jpg\n",
    "│   └── ...\n",
    "├── class2/\n",
    "│   ├── image1.jpg\n",
    "│   ├── image2.jpg\n",
    "│   └── ...\n",
    "└── classN/\n",
    "    ├── image1.jpg\n",
    "    ├── image2.jpg\n",
    "    └── ...\n",
    "```\n",
    "\n",
    "On cada subcarpeta dins de dataset_root té el nom d'una classe, i dins de cada subcarpeta hi ha les imatges corresponents a aquella classe.\n",
    "\n",
    "#### Com funciona ImageFolder\n",
    "\n",
    "1. Etiquetes automàtiques: ImageFolder assigna una etiqueta numèrica a cada carpeta (classe) seguint l'ordre alfabètic dels noms de les carpetes.\n",
    "2. Transformacions: Pots afegir transformacions com ToTensor, Resize, Normalize, etc., per pre-processar les imatges en el moment de carregar-les.\n",
    "3. Dades i etiquetes: Cada vegada que crides un element del dataset, ImageFolder retorna una tupla (imatge, etiqueta).\n",
    "\n",
    "Aquest mètode és molt eficient per carregar i estructurar imatges per a tasques de classificació i facilita la integració amb models de PyTorch com AlexNet, ResNet, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0756ea88b51da9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T16:24:08.055005Z",
     "start_time": "2024-10-30T16:24:08.008300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 64, 64]) tensor([134, 157, 194,  85])\n"
     ]
    }
   ],
   "source": [
    "img, target = next(iter(train_loader))\n",
    "print(img.shape, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe645c3e13180bbe",
   "metadata": {},
   "source": [
    "## Definicio de la xarxa: AlexNet i *Transfer learning*\n",
    "\n",
    "En aquesta pràctica aplicarem la tècnica de transfer learning amb la primera xarxa CNN moderna:\n",
    "- AlexNet. [ImageNet Classification with Deep Convolutional Neural Network, 2012](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf). La mida d'entrada de les imatges és de (227x227x3).Té prop de 60 milions de paràmetres entrenables.\n",
    "\n",
    "Pytorch ens permet emprar aquest tipus de xarxes de manera molt senzilla. [Més informació](https://pytorch.org/vision/stable/models.html). Si el model que cercam no es troba integrat dins la llibreria Pytorch és bastant probable que si la trobem a Huggingface.\n",
    "\n",
    "Descarregarem AlexNet i a analitzar-la. En aquest cas no només ens baixam la seva arquitectura, també els pesos resultants de l'entrenament.\n",
    "\n",
    "**Normalment els problems els resoldrem emprant models ja definits i preentrenats**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55fbbcc900043cba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T16:24:08.740844Z",
     "start_time": "2024-10-30T16:24:08.094689Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\miniconda3\\envs\\ia_2024\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Arquitectura AlexNet\n",
      "--------------------------------------------------\n",
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "alex = models.alexnet(weights=True)\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"Arquitectura AlexNet\")\n",
    "print(\"-\"*50)\n",
    "print(alex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633321e56ea811ed",
   "metadata": {},
   "source": [
    "Podem accedir a una capa concreta pel seu nom o índex dins de l’estructura del model. Per exemple: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cc3f9f3c6064fa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T16:24:08.773921Z",
     "start_time": "2024-10-30T16:24:08.763210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex.features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee807ab8ee2cdf1",
   "metadata": {},
   "source": [
    "El que nosaltres volem fer és emprar els pesos ja entrenats d'aquest model i aplicar-ho per resoldre un problema nou. Hi ha diverses maneres de realitzar la tècnica de ``Transfer Learning``. Les dues les hem explicades a la introducció, en aquest cas però emprarem el ``transfer learning`` pròpiament dit. L'element principal és congelar les capes de l'extractor de característiques. Per fer-ho empram les següents instruccions\n",
    "\n",
    "```\n",
    "for param in alex.features.parameters():\n",
    "   param.requires_grad = False\n",
    "\n",
    "```\n",
    "\n",
    "## Feina a fer:\n",
    " \n",
    "1. Carregar la xarxa AlexNet i congelar l'extractor de característiques.\n",
    "2. Definir un entorn seqüencial on implementarem el classificador de la xarxa.\n",
    "3. Realitzar un entrenament: comparar rendiment (accuracy) i nombre de paràmetres.\n",
    "4. Provar de guardar la vostra xarxa i tornar-la a carregar. Classificar una imatge del conjunt de test.\n",
    "\n",
    "**Nota**. Com veureu no us donam aquesta vegada el bucle d'entrenament, sigui com sigui podeu adaptar el vist a les sessions anteriors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41cae526ac948721",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T16:24:08.802682Z",
     "start_time": "2024-10-30T16:24:08.799720Z"
    }
   },
   "outputs": [],
   "source": [
    "alex = models.alexnet(weights=True)\n",
    "\n",
    "alex.classifier = nn.Sequential(\n",
    "    torch.nn.Linear(9216, 1024),\n",
    "    nn.ReLU(),\n",
    "    torch.nn.Linear(1024, 1024),\n",
    "    nn.ReLU(),\n",
    "    torch.nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    torch.nn.Linear(512, 200),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "for param in alex.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab1ae82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-3  # Hiperparàmetre\n",
    "optimizer = optim.Adam(alex.parameters(), lr=learning_rate)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = alex.to(device)\n",
    "\n",
    "running_loss = []\n",
    "running_acc = []\n",
    "\n",
    "running_test_loss = []\n",
    "running_test_acc_cnn = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fbb1e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches (Època 1): 6448it [03:40, 29.18it/s]s]\n",
      "Èpoques:   0%|          | 0/5 [03:40<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m i_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Iteram els batches.\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBatches (Època \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mt\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43malex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Posam el model a mode entranament.\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\miniconda3\\envs\\ia_2024\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\alber\\miniconda3\\envs\\ia_2024\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\alber\\miniconda3\\envs\\ia_2024\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\alber\\miniconda3\\envs\\ia_2024\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\alber\\miniconda3\\envs\\ia_2024\\Lib\\site-packages\\torchvision\\datasets\\folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32mc:\\Users\\alber\\miniconda3\\envs\\ia_2024\\Lib\\site-packages\\torchvision\\datasets\\folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\miniconda3\\envs\\ia_2024\\Lib\\site-packages\\torchvision\\datasets\\folder.py:262\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_loader\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    263\u001b[0m         img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for t in tqdm(range(EPOCHS), desc=\"Èpoques\"):\n",
    "    batch_loss = 0\n",
    "    batch_acc = 0\n",
    "    \n",
    "    i_batch = 1\n",
    "    # Iteram els batches.\n",
    "    for i_batch, (x, y) in tqdm(enumerate(train_loader), desc=f\"Batches (Època {t + 1})\"):\n",
    "        alex.train()  # Posam el model a mode entranament.\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1. PREDICCIÓ\n",
    "\n",
    "        y_pred = alex(x.to(device))\n",
    "\n",
    "        # 2. CALCUL DE LA PÈRDUA\n",
    "        # Computa la pèrdua: l'error de predicció vs el valor correcte\n",
    "        # Es guarda la pèrdua en un array per futures visualitzacions\n",
    "\n",
    "        loss = loss_fn(y_pred, y.to(device))\n",
    "\n",
    "        #3. GRADIENT\n",
    "        alex.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Actualitza els pesos utilitzant l'algorisme d'actualització\n",
    "        #4. OPTIMITZACIO\n",
    "        with torch.no_grad():\n",
    "            optimizer.step()\n",
    "\n",
    "        # 5. EVALUAM EL MODEL\n",
    "        alex.eval()  # Mode avaluació de la xarxa\n",
    "\n",
    "        y_pred = alex(x.to(device))\n",
    "        batch_loss += (loss_fn(y_pred, y.to(device)).detach())\n",
    "\n",
    "        y_pred_class = torch.argmax(y_pred, dim=1).detach().cpu().numpy()\n",
    "        batch_acc += accuracy_score(y.cpu().numpy(), y_pred_class)\n",
    "\n",
    "\n",
    "    running_loss.append(batch_loss / (i_batch + 1))\n",
    "    running_acc.append(batch_acc / (i_batch + 1))\n",
    "\n",
    "    batch_test_loss = 0\n",
    "    batch_test_acc = 0\n",
    "\n",
    "    for i_batch, (x, y) in enumerate(test_loader):\n",
    "\n",
    "        y_pred = alex(x.to(device))\n",
    "        batch_test_loss += (loss_fn(y_pred, y.to(device)).detach())\n",
    "        y_pred_class = torch.argmax(y_pred, dim=1).detach().cpu().numpy()\n",
    "        acc = accuracy_score(y.cpu().numpy(), y_pred_class)\n",
    "        batch_test_acc += acc\n",
    "\n",
    "    running_test_loss.append(batch_test_loss / (i_batch + 1))\n",
    "    running_test_acc_cnn.append(batch_test_acc / (i_batch + 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
