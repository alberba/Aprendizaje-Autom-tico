{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ab17735b9846e45",
   "metadata": {
    "id": "2ab17735b9846e45"
   },
   "source": [
    "# Segmentació d’Imatges amb U-Net\n",
    "\n",
    "En aquesta pràctica treballarem amb U-Net, una de les arquitectures més utilitzades en tasques de segmentació d'imatges. L'objectiu principal és consolidar els coneixements teòrics adquirits, implementant i experimentant amb aquesta xarxa utilitzant un conjunt de dades artificial i simple.\n",
    "\n",
    "La segmentació d'imatges és una tasca fonamental en visió per computador, ja que permet classificar cada píxel d'una imatge en una o més categories. En aquesta pràctica, el nostre objectiu serà entrenar una xarxa U-Net perquè pugui identificar regions específiques dins d’imatges generades artificialment. Aquest enfocament simplificat permetrà concentrar-nos en els aspectes clau de la implementació i el funcionament del model, sense les complicacions que podrien sorgir amb conjunts de dades més complexos.\n",
    "\n",
    "El conjunt de dades que utilitzarem estarà format per imatges sintètiques que contenen formes geomètriques bàsiques (com cercles, quadrats o creus), amb les seves corresponents màscares que indiquen les àrees que cal segmentar. Això ens permetrà obtenir resultats visuals ràpidament i entendre millor com la xarxa aprèn a identificar patrons específics.\n",
    "\n",
    "L'arquitectura que emprarem es pot utilitzar, per problemes més complexes, però amb les limitacions temporals que tenim no es recomenable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from IPython import display\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b534bb70a012f",
   "metadata": {
    "id": "3c4b534bb70a012f"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Emprarem un dataset sintètic. Aquest originalment s'emprava per dur a terme tasques de Intel·ligència Artificial Explicable (XAI). Nosaltres ho emprarem per dur a terme una tasca de segmentació. Podeu trobar el dataset complet al següent [enllaç](https://github.com/miquelmn/aixi-dataset/releases/tag/1.5.0).\n",
    "\n",
    "![Imatges dataset](https://ars.els-cdn.com/content/image/1-s2.0-S0004370224001152-gr001.jpg)\n",
    "\n",
    "L'objectiu amb aquest dataset es segmentar del fons les formes geomètriques, emprant una U-Net. Nosaltres i a causa de les limitacions temporals que tenim i de recursos emprarem una versió [reduida](https://github.com/bmalcover/aa_2425/releases/download/v1/mini.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "w-fsUWmZ7bat",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-fsUWmZ7bat",
    "outputId": "d10ab753-668c-4294-c21e-ca1943d74926"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n",
      "\"unzip\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    }
   ],
   "source": [
    "# !wget https://github.com/miquelmn/aixi-dataset/releases/download/1.5.0/TXUXIv3.zip\n",
    "!wget https://github.com/bmalcover/aa_2425/releases/download/v1/mini.zip\n",
    "!unzip mini.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5306dc4eb2351c",
   "metadata": {
    "id": "aa5306dc4eb2351c"
   },
   "source": [
    "# Dataset de segmentació\n",
    "\n",
    "A l’hora de treballar amb tasques de segmentació d’imatges, és essencial tenir un conjunt de dades que inclogui tant les imatges d’entrada com les màscares que representen les etiquetes de segmentació. Un dataset per segmentació ha de contenir:\n",
    "\n",
    "1. Imatges d’entrada: Aquestes són les imatges que el model utilitzarà per aprendre. Poden estar en formats com JPEG o PNG.\n",
    "2. Màscares de segmentació: Cada màscara és una imatge on cada píxel té un valor que representa la classe a la qual pertany (per exemple, 0 per fons, 1 per objecte). Les màscares han de tenir la mateixa mida que les imatges d’entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec59e1a421af006d",
   "metadata": {
    "id": "ec59e1a421af006d"
   },
   "outputs": [],
   "source": [
    "class Formes(Dataset):\n",
    "    \"\"\"TXUXI segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, images, labels, transform):\n",
    "        super().__init__()\n",
    "        self.paths = images\n",
    "        self.labels = labels\n",
    "        self.len = len(self.paths)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)  # Depén de vosaltres\n",
    "        label_img = cv2.imread(label, cv2.IMREAD_COLOR)\n",
    "        label_img = cv2.add(label_img[:, :, 0],\n",
    "                            cv2.add(label_img[:, :, 1], label_img[:, :, 2]))  # Depén de vosaltres\n",
    "\n",
    "        image = self.transform(image)\n",
    "        label_img = self.transform(label_img)\n",
    "\n",
    "        if label_img.max() > 0:\n",
    "            label_img = label_img / label_img.max()\n",
    "\n",
    "        return image, label_img  # Gran diferència"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e69d95c3109be79",
   "metadata": {
    "id": "2e69d95c3109be79"
   },
   "source": [
    "Per instanciar aquest tipus de dataset es similar a com ho hem fet fins ara. **Per tal de simplificar l'entrenament només emprarem 500 mostres per l'entrenament i 200 de test**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccaa997d1b472a80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ccaa997d1b472a80",
    "outputId": "7dae1f96-90c8-409e-f0f5-a71ddd477f7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "total training images 0\n",
      "total test images 0\n"
     ]
    }
   ],
   "source": [
    "PATH_DADES = \"./13_UNET/mini\"  # POSA EL TEU!\n",
    "# Dades entrenament\n",
    "path_train = f\"{PATH_DADES}/train\"\n",
    "\n",
    "img_files = sorted(glob(path_train + \"/image/*.png\"))\n",
    "print(img_files)\n",
    "label_files = sorted(glob(path_train + \"/mask/*.png\"))\n",
    "img_files = img_files[:500]\n",
    "label_files = label_files[:500]\n",
    "\n",
    "print(\"total training images\", len(img_files))\n",
    "\n",
    "# Dades validacio\n",
    "\n",
    "path_val = f\"{PATH_DADES}/val\"\n",
    "img_files_val = sorted(glob(path_val + \"/image/*.png\"))\n",
    "label_files_val = sorted(glob(path_val + \"/mask/*.png\"))\n",
    "img_files_val = img_files_val[:200]\n",
    "label_files_val = label_files_val[:200]\n",
    "\n",
    "print(\"total test images\", len(img_files_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70897b03ccb37f2c",
   "metadata": {
    "id": "70897b03ccb37f2c"
   },
   "outputs": [],
   "source": [
    "train_batch_size = 4\n",
    "test_batch_size = 4\n",
    "\n",
    "# Definim una seqüència (composició) de transformacions\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    ## TODO: Put if necessary\n",
    "])\n",
    "\n",
    "train_data = Formes(img_files, label_files, transform)\n",
    "val_data = Formes(img_files_val, label_files_val, transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, train_batch_size)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d5bdba44fdcc0f",
   "metadata": {
    "id": "b5d5bdba44fdcc0f"
   },
   "source": [
    "## Entrenament\n",
    "L’entrenament d’una xarxa U-Net consisteix a ensenyar al model a predir les màscares de segmentació corresponents a les imatges d’entrada. Aquest procés implica ajustar els pesos de la xarxa per minimitzar l’error entre les prediccions i les etiquetes reals (màscares). En aquesta secció, configurarem tot el necessari per entrenar el model amb el conjunt de dades generat prèviament.\n",
    "\n",
    "Hi ha tot un conjunt de peculiaritats que fan que aquest entrenament difereixi respecte els vists fins ara:\n",
    "- **Funció de pèrdua**. El Dice Coefficient és una mesura d’avaluació utilitzada en segmentació d’imatges per comparar la superposició entre la màscara predita i la màscara real. El seu valor oscil·la entre 0 (cap coincidència) i 1 (coincidència perfecta), i es calcula com el doble de la intersecció entre les dues màscares dividit per la seva suma total. També es pot emprar ``BCE``.\n",
    "- **Sortida de la xarxa**. El tipus de problema determina el nombre de canals de sortida de la U-Net: per segmentació binària, la sortida és un únic canal amb valors que representen la probabilitat de pertànyer a la classe positiva (fons o objecte). En canvi, per segmentació multiclasse, la sortida té un canal per a cada classe i els valors representen la probabilitat de cada píxel de pertànyer a cadascuna de les classes.\n",
    "- **Funcions d'activació**. Les funcions d'activació a la sortida d'una U-Net depenen del tipus de segmentació: en segmentació binària, s'utilitza una sigmoid per comprimir els valors entre 0 i 1, representant la probabilitat de pertànyer a la classe positiva. En segmentació, **sense superposició**, multiclasse, s'aplica una softmax per convertir els valors de cada canal en probabilitats normalitzades, assegurant que la suma sigui 1 per píxel.\n",
    "\n",
    "### El model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63702cf4a0766c0b",
   "metadata": {
    "id": "63702cf4a0766c0b"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        features = init_features\n",
    "\n",
    "        ## CODER\n",
    "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
    "\n",
    "        ## DECODER\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2,\n",
    "                                          stride=2)  # Empra aquesta capa com exemple\n",
    "        self.decoder4 = UNet._block(features * 16, features * 8, name=\"dec4\")\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n",
    "        self.decoder3 = UNet._block(features * 8, features * 4, name=\"dec3\")\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n",
    "        self.decoder2 = UNet._block(features * 4, features * 2, name=\"dec2\")\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n",
    "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
    "\n",
    "        self.final = nn.Conv2d(\n",
    "            in_channels=features,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=1,\n",
    "            padding=0,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        dec1 = self.upconv4(bottleneck)\n",
    "        dec1 = torch.cat((dec1, enc4), dim=1)\n",
    "        dec2 = self.decoder4(dec1)\n",
    "\n",
    "        dec2 = self.upconv3(dec2)\n",
    "        dec2 = torch.cat((dec2, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec2)\n",
    "\n",
    "        dec3 = self.upconv2(dec3)\n",
    "        dec3 = torch.cat((dec3, enc2), dim=1)\n",
    "        dec4 = self.decoder2(dec3)\n",
    "\n",
    "        dec4 = self.upconv1(dec4)\n",
    "        dec4 = torch.cat((dec4, enc1), dim=1)\n",
    "        dec5 = self.decoder1(dec4)\n",
    "        return torch.sigmoid(self.final(dec5))\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features, name):\n",
    "        return nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (name + \"conv1\",\n",
    "                     nn.Conv2d(\n",
    "                         in_channels=in_channels,\n",
    "                         out_channels=features,\n",
    "                         kernel_size=3,\n",
    "                         padding=1,\n",
    "                         bias=False,\n",
    "                     ),\n",
    "                     ),\n",
    "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
    "                    (name + \"conv2\",\n",
    "                     nn.Conv2d(\n",
    "                         in_channels=features,\n",
    "                         out_channels=features,\n",
    "                         kernel_size=3,\n",
    "                         padding=1,\n",
    "                         bias=False,\n",
    "                     ),\n",
    "                     ),\n",
    "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
    "                ]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45ce45fc4019d2a",
   "metadata": {
    "id": "c45ce45fc4019d2a"
   },
   "source": [
    "## La funció de pèrdua\n",
    "\n",
    "Per fer tasques de segmentació, una de les funcions de pèrdua que podem emprar és el _Diceloss_ (intersecció vs unió):  El coeficient de _Dice_ s'utilitza habitualment en tasques de segmentació d'imatges com a mesura de la superposició entre les màscares de segmentació entre la predicció i el _ground truth_. El  _Diceloss_, és el complementari del coeficient de _Dice_, es pot utilitzar com a funció de pèrdua per entrenar models per a tasques de segmentació.\n",
    "\n",
    "Dice Coefficient $= 2 \\times \\frac{|X \\cap Y|}{|X| + |Y|}$\n",
    "\n",
    "\n",
    "\n",
    "On:\n",
    "\n",
    "- $X$ és la màscara de segmentació prevista.\n",
    "- $Y$ és la màscara de segmentació de la veritat del sòl.\n",
    "- $∣⋅∣$ denota la cardinalitat o el nombre d'elements d'un conjunt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcdfff5c808b553",
   "metadata": {
    "id": "2fcdfff5c808b553"
   },
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = 0.0\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        assert y_pred.size() == y_true.size()\n",
    "        y_pred = y_pred[:, 0].contiguous().view(-1)\n",
    "        y_true = y_true[:, 0].contiguous().view(-1)\n",
    "        intersection = (y_pred * y_true).sum()\n",
    "        dsc = (2. * intersection + self.smooth) / (\n",
    "                y_pred.sum() + y_true.sum() + self.smooth\n",
    "        )\n",
    "        return 1. - dsc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a610253071a5c8a",
   "metadata": {
    "id": "1a610253071a5c8a"
   },
   "source": [
    "### Bucle d'entrenament\n",
    "\n",
    "El bucle d'entrenament és un poc diferent al vist fins ara. En particular, en aquests moments els gràfics es van actualitzant a mesura que aprèn i així podem tenir un idea de cm va l'entrenament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba7aa7b0f9559c4",
   "metadata": {
    "id": "8ba7aa7b0f9559c4"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 40\n",
    "\n",
    "model = UNet(1, 1).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = DiceLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635827cc7b0906b3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725,
     "referenced_widgets": [
      "ac72d0580cc04643b68aa142e3de49eb",
      "8467bca2a59847bba97084aa6d44d6fd",
      "fcaea5a9985b4df49b3b28cbdb5f1d14",
      "9ee47ca8dc274234b894ae197438b23f",
      "6ea773274da6472d85f5c01f74136f36",
      "3c296fa64eb644c994f058968c6a36d5",
      "87cc495bffff410db938c19d4dbd5a10",
      "21f286946d6d4f53914ad32f1bd4b480",
      "f236cb08270d49aea37cd76bfd7b13f4",
      "d942f0f6dcd04309b5711ef7e817a943",
      "812c385db47e42fc8943844710df000f"
     ]
    },
    "id": "635827cc7b0906b3",
    "outputId": "8b695399-cbdf-4906-fdb7-4321cee3ef6c"
   },
   "outputs": [],
   "source": [
    "t_loss = np.zeros(epochs)\n",
    "v_loss = np.zeros(epochs)\n",
    "\n",
    "pbar = tqdm(range(1, epochs + 1))  # tdqm permet tenir text dinàmic\n",
    "\n",
    "for epoch in pbar:\n",
    "\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for batch_num, (input_img, target) in enumerate(train_loader, 1):\n",
    "        input_img = input_img.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(input_img)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for input_img, target in val_loader:\n",
    "            input_img = input_img.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(input_img)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # RESULTATS\n",
    "    train_loss /= len(train_loader)\n",
    "    t_loss[epoch - 1] = train_loss\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    v_loss[epoch - 1] = val_loss\n",
    "\n",
    "    # VISUALITZACIO DINAMICA\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    pl.plot(t_loss[:epoch], label=\"train\")\n",
    "    pl.plot(v_loss[:epoch], label=\"validation\")\n",
    "    pl.legend()\n",
    "    pl.xlim(0, epochs)\n",
    "    pl.xticks(range(0, epochs, 1), range(1, epochs + 1, 1))\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(pl.gcf())\n",
    "    plt.close()\n",
    "\n",
    "    pbar.set_description(f\"Epoch:{epoch} Training Loss:{train_loss} Validation Loss:{val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GxiqCgaOts1n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "GxiqCgaOts1n",
    "outputId": "9bfab8d4-7a7e-4d7e-84db-c351821be4e0"
   },
   "outputs": [],
   "source": [
    "img, mask = next(iter(val_loader))\n",
    "res = model(img.cuda()).detach().cpu().numpy()\n",
    "\n",
    "plt.figure(figsize = (15, 10))\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "ax.title.set_text('Imatge')\n",
    "plt.imshow(img[1, 0, :, :]);\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "ax.title.set_text('GT')\n",
    "plt.imshow(mask[1, 0, :, :].detach().cpu().numpy());\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "ax.title.set_text('Predicció')\n",
    "plt.imshow(res[1, 0, :, :]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c28fc08fda202c1",
   "metadata": {},
   "source": [
    "# Tasques a fer\n",
    "\n",
    "1. Que passa si canviam la funció de pèrdua i ara empram ``BCE``?\n",
    "2. Fer un nou entrenament per segmentar de forma separada els diferents tipus de figura. Has d'adaptar el model. Quins canvis faries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c063c6af21382cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ia_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "21f286946d6d4f53914ad32f1bd4b480": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c296fa64eb644c994f058968c6a36d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ea773274da6472d85f5c01f74136f36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "812c385db47e42fc8943844710df000f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8467bca2a59847bba97084aa6d44d6fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c296fa64eb644c994f058968c6a36d5",
      "placeholder": "​",
      "style": "IPY_MODEL_87cc495bffff410db938c19d4dbd5a10",
      "value": "Epoch:7 Training Loss:0.03439910078048706 Validation Loss:0.07547110319137573:  18%"
     }
    },
    "87cc495bffff410db938c19d4dbd5a10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9ee47ca8dc274234b894ae197438b23f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d942f0f6dcd04309b5711ef7e817a943",
      "placeholder": "​",
      "style": "IPY_MODEL_812c385db47e42fc8943844710df000f",
      "value": " 7/40 [00:32&lt;02:21,  4.28s/it]"
     }
    },
    "ac72d0580cc04643b68aa142e3de49eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8467bca2a59847bba97084aa6d44d6fd",
       "IPY_MODEL_fcaea5a9985b4df49b3b28cbdb5f1d14",
       "IPY_MODEL_9ee47ca8dc274234b894ae197438b23f"
      ],
      "layout": "IPY_MODEL_6ea773274da6472d85f5c01f74136f36"
     }
    },
    "d942f0f6dcd04309b5711ef7e817a943": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f236cb08270d49aea37cd76bfd7b13f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fcaea5a9985b4df49b3b28cbdb5f1d14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21f286946d6d4f53914ad32f1bd4b480",
      "max": 40,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f236cb08270d49aea37cd76bfd7b13f4",
      "value": 7
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
