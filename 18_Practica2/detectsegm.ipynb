{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesadas 152 imágenes y 152 máscaras.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import cv2\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import torch\n",
    "import pylab as pl\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision import tv_tensors\n",
    "from tqdm.auto import tqdm\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "def process_images_and_masks(base_dir_images, base_dir_annotations):\n",
    "    \"\"\"\n",
    "    Procesa imágenes y máscaras de las clases 'buddha' y 'dalmatian'.\n",
    "\n",
    "    Args:\n",
    "        base_dir_images (str): Ruta base de las imágenes (101_ObjectCategories).\n",
    "        base_dir_annotations (str): Ruta base de las anotaciones.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de imágenes procesadas como arrays de NumPy.\n",
    "        list: Lista de máscaras procesadas como arrays de NumPy.\n",
    "    \"\"\"\n",
    "    classes_to_process = {'buddha': 1, 'dalmatian': 2}\n",
    "    image_list = []\n",
    "    mask_list = []\n",
    "    class_list = []\n",
    "\n",
    "    for class_name, class_id in classes_to_process.items():\n",
    "        # Rutas de las subcarpetas\n",
    "        class_images_dir = os.path.join(base_dir_images, class_name)\n",
    "        class_annotations_dir = os.path.join(base_dir_annotations, class_name)\n",
    "\n",
    "        # Verifica si las carpetas existen\n",
    "        if not os.path.isdir(class_images_dir) or not os.path.isdir(class_annotations_dir):\n",
    "            print(f\"Carpeta no encontrada: {class_name}\")\n",
    "            continue\n",
    "\n",
    "        # Procesar las imágenes y las anotaciones\n",
    "        for annotation_file in os.listdir(class_annotations_dir):\n",
    "            if annotation_file.endswith('.mat'):\n",
    "                annotation_path = os.path.join(class_annotations_dir, annotation_file)\n",
    "                annotation_id = os.path.splitext(annotation_file)[0].split('_')[-1]\n",
    "\n",
    "                # Ruta de la imagen correspondiente\n",
    "                image_file = f\"image_{annotation_id}.jpg\"\n",
    "                image_path = os.path.join(class_images_dir, image_file)\n",
    "\n",
    "                if not os.path.exists(image_path):\n",
    "                    print(f\"Imagen no encontrada para {annotation_file}\")\n",
    "                    continue\n",
    "\n",
    "                # Cargar la anotación\n",
    "                mat_data = scipy.io.loadmat(annotation_path)\n",
    "                if 'obj_contour' not in mat_data:\n",
    "                    print(f\"'obj_contour' no encontrada en {annotation_file}\")\n",
    "                    continue\n",
    "\n",
    "                obj_contour = mat_data['obj_contour']  # Contorno de la máscara\n",
    "\n",
    "                # Cargar la imagen\n",
    "                image = cv2.imread(image_path)\n",
    "                if image is None:\n",
    "                    print(f\"Error al cargar la imagen {image_path}\")\n",
    "                    continue\n",
    "\n",
    "                # Crear una máscara vacía del mismo tamaño que la imagen\n",
    "                mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "                boxcoord = mat_data['box_coord'].flatten()\n",
    "\n",
    "                # Dibujar el contorno en la máscara\n",
    "                contour_points = np.transpose(obj_contour)  # Shape (Y, 2)\n",
    "                contour_points[:, 0] += boxcoord[2]-1.75  # Ajustar X según x0 del bounding box\n",
    "                contour_points[:, 1] += boxcoord[0]-2  # Ajustar Y según y0 del bounding box\n",
    "                contour_points = contour_points.astype(np.int32)  # Asegurar tipo correcto\n",
    "\n",
    "                cv2.drawContours(mask, [contour_points], contourIdx=-1, color=255, thickness=-1)\n",
    "\n",
    "                # Agregar la imagen y la máscara a las listas\n",
    "                image_list.append(image)\n",
    "                mask_list.append(mask)\n",
    "                class_list.append(class_id)\n",
    "\n",
    "    return image_list, mask_list, class_list\n",
    "\n",
    "base_dir_images = \"./dataAux/caltech101/101_ObjectCategories\"\n",
    "base_dir_annotations = \"./dataAux/caltech101/Annotations\"\n",
    "images, masks, classes = process_images_and_masks(base_dir_images, base_dir_annotations)\n",
    "print(f\"Procesadas {len(images)} imágenes y {len(masks)} máscaras.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(images, masks, classes):\n",
    "    \"\"\"\n",
    "    Realiza aumentos de datos en las imágenes y máscaras.\n",
    "\n",
    "    Args:\n",
    "        images (list): Lista de imágenes como arrays de NumPy.\n",
    "        masks (list): Lista de máscaras como arrays de NumPy.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de imágenes aumentadas como arrays de NumPy.\n",
    "        list: Lista de máscaras aumentadas como arrays de NumPy.\n",
    "    \"\"\"\n",
    "    augmented_images = []\n",
    "    augmented_masks = []\n",
    "    augmented_classes = []\n",
    "\n",
    "    # Transformaciones para imágenes y máscaras\n",
    "    transform = transforms.Compose([\n",
    "        # 1) Ajuste del tamaño a 256x256\n",
    "        transforms.Resize((256, 256)),\n",
    "\n",
    "        # 2) Recorte aleatorio para 224x224, simulando diferentes encuadres/zoom\n",
    "        transforms.RandomResizedCrop(\n",
    "            size=256,\n",
    "            scale=(0.8, 1.0),       # rango de escalado\n",
    "            ratio=(0.9, 1.1)        # rango de aspecto (ancho x alto)\n",
    "        ),\n",
    "\n",
    "        # 3) Rotación aleatoria moderada (±15 grados)\n",
    "        transforms.RandomRotation(degrees=30),\n",
    "\n",
    "        # 4) Flip horizontal aleatorio (probabilidad 0.5)\n",
    "        transforms.RandomHorizontalFlip(p=1),\n",
    "\n",
    "        # 5) Ajustes de color para simular distintas condiciones de iluminación\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.5, \n",
    "            contrast=0.2, \n",
    "            saturation=0.4, \n",
    "            hue=0.05\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "\n",
    "    num_nuevas_por_imagen = 2\n",
    "    for img, mask, img_class in zip(images, masks, classes):\n",
    "        # Convertir la imagen y la máscara a PIL\n",
    "        img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        mask_pil = Image.fromarray(mask)\n",
    "\n",
    "        for i in range(num_nuevas_por_imagen):\n",
    "\n",
    "            # Aplicar las transformaciones\n",
    "            seed = random.randint(0, 2**32)  # Para asegurar que las mismas transformaciones se aplican a ambos\n",
    "            random.seed(seed)\n",
    "            augmented_img = transform(img_pil)\n",
    "            random.seed(seed)\n",
    "            augmented_mask = transform(mask_pil)\n",
    "\n",
    "            # Convertir de nuevo a arrays de NumPy\n",
    "            augmented_images.append(np.array(augmented_img))\n",
    "            augmented_masks.append(np.array(augmented_mask))\n",
    "            augmented_classes.append(img_class)\n",
    "\n",
    "    return augmented_images, augmented_masks, augmented_classes\n",
    "\n",
    "augmented_images, augmented_masks, augmented_classes = augment_data(images, masks, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset personalizado para imágenes y máscaras, compatible con Mask R-CNN.\n",
    "    \"\"\"\n",
    "    def __init__(self, images, masks, classes, transforms=None):\n",
    "        \"\"\"\n",
    "        Inicializa el dataset.\n",
    "\n",
    "        Args:\n",
    "            images (list): Lista de imágenes como arrays de NumPy.\n",
    "            masks (list): Lista de máscaras como arrays de NumPy.\n",
    "            classes (list): Lista de etiquetas de clase correspondientes.\n",
    "            transform (callable, optional): Transformaciones a aplicar a las imágenes y máscaras.\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        print(len(images))\n",
    "        self.masks = masks\n",
    "        self.classes = classes\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "        mask = np.expand_dims(mask, axis=0)  # Añadir canal para compatibilidad con Torch\n",
    "        mask = torch.Tensor(mask)\n",
    "        print(\"Dimensiones de la imagen:\", image.shape)\n",
    "        print(\"Dimensiones de la mascara:\", mask.shape)\n",
    "        label = self.classes[idx]\n",
    "\n",
    "        num_objs = 1\n",
    "\n",
    "        # Crear el target para Mask R-CNN\n",
    "        boxes = masks_to_boxes(mask)\n",
    "        area = (boxes[0][2] - boxes[0][0]) * (boxes[0][3] - boxes[0][1])\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        img = tv_tensors.Image(image)\n",
    "\n",
    "        print(\"Dimensiones de la imagen antes de transformaciones:\", img.shape)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = tv_tensors.BoundingBoxes(\n",
    "            boxes, format=\"XYXY\", canvas_size=F.get_size(img)\n",
    "        )\n",
    "        target[\"masks\"] =  tv_tensors.Mask(mask)\n",
    "        target[\"labels\"] = torch.tensor([label], dtype=torch.int64)\n",
    "        target[\"image_id\"] = idx\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        print(\"Dimensiones de la imagen después de transformaciones:\", img.shape)\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304\n"
     ]
    }
   ],
   "source": [
    "transforms = T.Compose(\n",
    "    [\n",
    "        T.ToDtype(torch.float, scale=True),\n",
    "        T.ToPureTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = CustomDataset(augmented_images, augmented_masks, augmented_classes, transforms)\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "  model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "  in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "  hidden_layer = model.roi_heads.mask_predictor.conv5_mask.out_channels\n",
    "\n",
    "  # and replace the mask predictor with a new one\n",
    "  model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "      in_features_mask, hidden_layer, num_classes\n",
    "  )\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model_instance_segmentation(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m      9\u001b[0m t_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(epochs)\n",
      "File \u001b[1;32mc:\\Users\\alber\\miniconda3\\envs\\ia_2024\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\miniconda3\\envs\\ia_2024\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alber\\miniconda3\\envs\\ia_2024\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alber\\miniconda3\\envs\\ia_2024\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alber\\miniconda3\\envs\\ia_2024\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alber\\miniconda3\\envs\\ia_2024\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1321\u001b[0m             device,\n\u001b[0;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m             non_blocking,\n\u001b[0;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1325\u001b[0m         )\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = get_model_instance_segmentation(2)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "t_loss = np.zeros(epochs)\n",
    "v_loss = np.zeros(epochs)\n",
    "\n",
    "pbar = tqdm(range(1, epochs + 1))  # tdqm permet tenir text dinàmic\n",
    "\n",
    "for epoch in pbar:\n",
    "\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for images, targets in data_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [\n",
    "            {\n",
    "                k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                for k, v in t.items()\n",
    "            } for t in targets\n",
    "        ]\n",
    "\n",
    "        images = [image.permute(2, 0, 1) for image in images]\n",
    "        print(\"Shape de la imagen:\", images[0].shape)\n",
    "        print(targets[0][\"masks\"].shape)\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += losses.item()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    t_loss[epoch - 1] = train_loss\n",
    "\n",
    "    pl.figure(figsize=(12, 4))\n",
    "    pl.plot(t_loss[:epoch], label=\"train\")\n",
    "    pl.legend()\n",
    "    pl.xlim(0, epochs)\n",
    "    pl.xticks(range(0, epochs, 1), range(1, epochs + 1, 1))\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(pl.gcf())\n",
    "    pl.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
